#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é“¶è¡Œè¡Œä¸šæ”¿ç­–çŸ¥è¯†åº“ç®€åŒ–æ··åˆæœç´¢å¼•æ“
ç»“åˆBM25å’ŒTF-IDFç®—æ³•çš„æ··åˆæœç´¢
"""

import json
import re
import math
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging
from collections import defaultdict, Counter

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimpleHybridSearchEngine:
    """ç®€åŒ–æ··åˆæœç´¢å¼•æ“ - ç»“åˆBM25å’ŒTF-IDF"""
    
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.index_path = self.base_path / "index"
        self.data_path = self.base_path / "data"
        
        # åŠ è½½ç´¢å¼•æ–‡ä»¶
        self.document_index = self._load_index("document_index.json")
        self.topic_index = self._load_index("topic_index.json")
        self.keyword_index = self._load_index("keyword_index.json")
        
        # åˆå§‹åŒ–æœç´¢ç›¸å…³å˜é‡
        self.documents = []
        self.document_contents = {}
        self.avg_doc_length = 0
        self.total_docs = 0
        self.doc_freq = defaultdict(int)
        self.term_freq = defaultdict(lambda: defaultdict(int))
        
        # æ„å»ºç´¢å¼•
        self._build_indices()
        
        logger.info("ç®€åŒ–æ··åˆæœç´¢å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def _load_index(self, filename: str) -> Dict[str, Any]:
        """åŠ è½½ç´¢å¼•æ–‡ä»¶"""
        try:
            index_file = self.index_path / filename
            with open(index_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½ç´¢å¼•æ–‡ä»¶ {filename} å¤±è´¥: {e}")
            return {}
    
    def _build_indices(self):
        """æ„å»ºæœç´¢ç´¢å¼•"""
        logger.info("å¼€å§‹æ„å»ºæœç´¢ç´¢å¼•...")
        
        for doc in self.document_index.get("documents", []):
            content = self._get_document_content(doc["id"])
            if content:
                self.documents.append(doc)
                self.document_contents[doc["id"]] = content
                
                words = self._tokenize_text(content)
                doc_length = len(words)
                
                word_freq = Counter(words)
                for word, freq in word_freq.items():
                    self.term_freq[doc["id"]][word] = freq
                    self.doc_freq[word] += 1
                
                self.avg_doc_length += doc_length
        
        self.total_docs = len(self.documents)
        if self.total_docs > 0:
            self.avg_doc_length /= self.total_docs
        
        logger.info(f"ç´¢å¼•æ„å»ºå®Œæˆ: {self.total_docs}ä¸ªæ–‡æ¡£, {len(self.doc_freq)}ä¸ªè¯é¡¹")
    
    def _tokenize_text(self, text: str) -> List[str]:
        """æ–‡æœ¬åˆ†è¯"""
        # ç®€å•çš„ä¸­æ–‡åˆ†è¯
        words = re.findall(r'[\u4e00-\u9fa5]+|[a-zA-Z]+|\d+', text)
        filtered_words = []
        for word in words:
            if len(word) > 1:
                filtered_words.append(word)
        return filtered_words
    
    def _get_document_content(self, document_id: str) -> Optional[str]:
        """è·å–æ–‡æ¡£å†…å®¹"""
        doc = None
        for d in self.document_index.get("documents", []):
            if d["id"] == document_id:
                doc = d
                break
        
        if not doc:
            return None
        
        try:
            file_path = self.base_path / doc["file_path"]
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logger.error(f"è¯»å–æ–‡æ¡£å†…å®¹å¤±è´¥: {e}")
            return None
    
    def _calculate_bm25_score(self, query: str, doc_id: str, k1: float = 1.2, b: float = 0.75) -> float:
        """è®¡ç®—BM25åˆ†æ•°"""
        query_words = self._tokenize_text(query)
        doc_words = self._tokenize_text(self.document_contents.get(doc_id, ""))
        doc_length = len(doc_words)
        
        score = 0.0
        
        for word in query_words:
            if word in self.term_freq[doc_id]:
                if self.doc_freq[word] > 0:
                    idf = math.log((self.total_docs - self.doc_freq[word] + 0.5) / 
                                 (self.doc_freq[word] + 0.5))
                else:
                    idf = 0
                
                tf = self.term_freq[doc_id][word]
                numerator = tf * (k1 + 1)
                denominator = tf + k1 * (1 - b + b * (doc_length / self.avg_doc_length))
                
                score += idf * (numerator / denominator)
        
        return score
    
    def _calculate_tfidf_score(self, query: str, doc_id: str) -> float:
        """è®¡ç®—TF-IDFåˆ†æ•°"""
        query_words = self._tokenize_text(query)
        doc_words = self._tokenize_text(self.document_contents.get(doc_id, ""))
        
        score = 0.0
        
        for word in query_words:
            if word in self.term_freq[doc_id]:
                # è®¡ç®—TF
                tf = self.term_freq[doc_id][word] / len(doc_words)
                
                # è®¡ç®—IDF
                if self.doc_freq[word] > 0:
                    idf = math.log(self.total_docs / self.doc_freq[word])
                else:
                    idf = 0
                
                score += tf * idf
        
        return score
    
    def hybrid_search(self, query: str, limit: int = 10, 
                     bm25_weight: float = 0.6, tfidf_weight: float = 0.4) -> List[Dict[str, Any]]:
        """æ··åˆæœç´¢"""
        results = []
        
        for doc in self.documents:
            doc_id = doc["id"]
            
            bm25_score = self._calculate_bm25_score(query, doc_id)
            tfidf_score = self._calculate_tfidf_score(query, doc_id)
            
            # å½’ä¸€åŒ–åˆ†æ•°
            bm25_score_norm = bm25_score / max(bm25_score, 1e-6)
            tfidf_score_norm = tfidf_score / max(tfidf_score, 1e-6)
            
            hybrid_score = bm25_weight * bm25_score_norm + tfidf_weight * tfidf_score_norm
            
            if hybrid_score > 0:
                context = self._extract_context(query, doc_id)
                
                result = {
                    "type": "hybrid_search",
                    "query": query,
                    "document_id": doc_id,
                    "title": doc["title"],
                    "author": doc.get("author", ""),
                    "publish_date": doc.get("publish_date", ""),
                    "hybrid_score": hybrid_score,
                    "bm25_score": bm25_score,
                    "tfidf_score": tfidf_score,
                    "context": context,
                    "summary": doc.get("summary", ""),
                    "keywords": doc.get("keywords", [])
                }
                results.append(result)
        
        results.sort(key=lambda x: x["hybrid_score"], reverse=True)
        return results[:limit]
    
    def _extract_context(self, query: str, doc_id: str) -> List[Dict[str, Any]]:
        """æå–æŸ¥è¯¢ç›¸å…³çš„ä¸Šä¸‹æ–‡"""
        content = self.document_contents.get(doc_id, "")
        if not content:
            return []
        
        contexts = []
        query_words = self._tokenize_text(query)
        paragraphs = content.split('\n')
        
        for i, para in enumerate(paragraphs):
            para_words = self._tokenize_text(para)
            
            if any(word in para_words for word in query_words):
                start = max(0, i - 1)
                end = min(len(paragraphs), i + 2)
                context_paras = paragraphs[start:end]
                
                context = {
                    "paragraph_index": i,
                    "content": para.strip(),
                    "context": '\n'.join(context_paras).strip(),
                    "relevance": sum(1 for word in query_words if word in para_words) / len(query_words)
                }
                contexts.append(context)
        
        contexts.sort(key=lambda x: x["relevance"], reverse=True)
        return contexts[:3]
    
    def search_by_keyword_hybrid(self, keyword: str, limit: int = 10) -> List[Dict[str, Any]]:
        """åŸºäºå…³é”®è¯çš„æ··åˆæœç´¢"""
        return self.hybrid_search(keyword, limit)
    
    def search_by_topic_hybrid(self, topic: str, limit: int = 10) -> List[Dict[str, Any]]:
        """åŸºäºä¸»é¢˜çš„æ··åˆæœç´¢"""
        topic_keywords = []
        topics = self.topic_index.get("topics", {})
        if topic in topics:
            topic_data = topics[topic]
            for subtopic, data in topic_data.get("subtopics", {}).items():
                topic_keywords.extend(data.get("key_terms", []))
        
        if not topic_keywords:
            topic_keywords = [topic]
        
        query = " ".join(topic_keywords)
        results = self.hybrid_search(query, limit)
        
        for result in results:
            result["topic"] = topic
            result["topic_keywords"] = topic_keywords
        
        return results
    
    def get_search_statistics(self) -> Dict[str, Any]:
        """è·å–æœç´¢ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_documents": self.total_docs,
            "total_terms": len(self.doc_freq),
            "avg_document_length": self.avg_doc_length,
            "index_size_mb": sum(len(content.encode('utf-8')) for content in self.document_contents.values()) / (1024 * 1024),
            "popular_terms": sorted(self.doc_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        }

def main():
    """æµ‹è¯•ç®€åŒ–æ··åˆæœç´¢å¼•æ“"""
    print("=== é“¶è¡Œè¡Œä¸šæ”¿ç­–çŸ¥è¯†åº“æ··åˆæœç´¢å¼•æ“ ===")
    
    try:
        search_engine = SimpleHybridSearchEngine(".")
        
        stats = search_engine.get_search_statistics()
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ–‡æ¡£æ€»æ•°: {stats['total_documents']}")
        print(f"   è¯é¡¹æ€»æ•°: {stats['total_terms']}")
        print(f"   å¹³å‡æ–‡æ¡£é•¿åº¦: {stats['avg_document_length']:.1f}è¯")
        print(f"   ç´¢å¼•å¤§å°: {stats['index_size_mb']:.2f}MB")
        
        print(f"\nğŸ”¥ çƒ­é—¨è¯é¡¹:")
        for term, freq in stats['popular_terms']:
            print(f"   {term}: {freq}æ¬¡")
        
        print(f"\nğŸ” æ··åˆæœç´¢æµ‹è¯•:")
        results = search_engine.hybrid_search("æ™®æƒ é‡‘è", 3)
        for i, result in enumerate(results, 1):
            print(f"\n{i}. {result['title']}")
            print(f"   æ··åˆåˆ†æ•°: {result['hybrid_score']:.4f}")
            print(f"   BM25åˆ†æ•°: {result['bm25_score']:.4f}")
            print(f"   TF-IDFåˆ†æ•°: {result['tfidf_score']:.4f}")
            print(f"   ä¸Šä¸‹æ–‡æ•°é‡: {len(result['context'])}")
        
        print(f"\nğŸ“š ä¸»é¢˜æ··åˆæœç´¢æµ‹è¯•:")
        results = search_engine.search_by_topic_hybrid("æ™®æƒ é‡‘è", 2)
        for i, result in enumerate(results, 1):
            print(f"\n{i}. {result['title']} (ä¸»é¢˜: {result['topic']})")
            print(f"   æ··åˆåˆ†æ•°: {result['hybrid_score']:.4f}")
        
        print(f"\nâœ… æ··åˆæœç´¢å¼•æ“æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        logger.error(f"æ··åˆæœç´¢å¼•æ“æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    main() 