#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TXTæ–‡ä»¶è½¬JSONæ ¼å¼åˆ‡åˆ†å·¥å…·
å°†extracted_textsç›®å½•ä¸‹çš„TXTæ–‡ä»¶è½¬æ¢ä¸ºJSONæ ¼å¼ï¼Œå¹¶æŒ‰ç…§æŒ‡å®šå‚æ•°è¿›è¡Œåˆ‡åˆ†
"""

import json
import os
import hashlib
from pathlib import Path
from typing import List, Dict, Any
import re


def split_text_file(file_path: Path, chunk_size: int = 30, chunk_overlap: int = 5) -> List[Dict[str, Any]]:
    """
    åˆ‡åˆ†å•ä¸ªæ–‡æœ¬æ–‡ä»¶
    
    Args:
        file_path: æ–‡æœ¬æ–‡ä»¶è·¯å¾„
        chunk_size: æ¯ä¸ªå—çš„æœ€å¤§è¡Œæ•°
        chunk_overlap: å—ä¹‹é—´çš„é‡å è¡Œæ•°
    
    Returns:
        åŒ…å«åˆ‡åˆ†å—çš„åˆ—è¡¨
    """
    chunks = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        total_lines = len(lines)
        
        # æŒ‰è¡Œåˆ‡åˆ†
        start_line = 1
        while start_line <= total_lines:
            end_line = min(start_line + chunk_size - 1, total_lines)
            
            # æå–å½“å‰å—çš„è¡Œ
            chunk_lines = lines[start_line - 1:end_line]
            chunk_text = ''.join(chunk_lines)
            
            # åˆ›å»ºå—å¯¹è±¡
            chunk = {
                "lines": [start_line, end_line],
                "text": chunk_text
            }
            
            chunks.append(chunk)
            
            # è®¡ç®—ä¸‹ä¸€ä¸ªå—çš„èµ·å§‹è¡Œï¼ˆè€ƒè™‘é‡å ï¼‰
            start_line = end_line - chunk_overlap + 1
            
            # é¿å…æ— é™å¾ªç¯
            if start_line > total_lines:
                break
    
    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        return []
    
    return chunks


def generate_sha1(file_path: Path) -> str:
    """ç”Ÿæˆæ–‡ä»¶çš„SHA1å“ˆå¸Œå€¼"""
    try:
        with open(file_path, 'rb') as f:
            content = f.read()
        return hashlib.sha1(content).hexdigest()
    except:
        return f"file_{file_path.stem}"


def extract_company_name(file_name: str) -> str:
    """ä»æ–‡ä»¶åä¸­æå–å…¬å¸/æœºæ„åç§°"""
    # ç§»é™¤æ–‡ä»¶æ‰©å±•å
    name = file_name.replace('_extracted.txt', '')
    
    # å°è¯•æå–æœºæ„åç§°
    patterns = [
        r'^(.+?)(?:æŠ¥å‘Š|å±•æœ›|åˆ†æ)',
        r'^(.+?)(?:\d{4}å¹´)',
        r'^(.+?)(?:202[0-9])',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, name)
        if match:
            return match.group(1).strip()
    
    return name


def convert_txt_to_json(input_dir: Path, output_dir: Path, chunk_size: int = 30, chunk_overlap: int = 5):
    """
    æ‰¹é‡è½¬æ¢TXTæ–‡ä»¶ä¸ºJSONæ ¼å¼
    
    Args:
        input_dir: è¾“å…¥ç›®å½•ï¼ˆåŒ…å«TXTæ–‡ä»¶ï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆä¿å­˜JSONæ–‡ä»¶ï¼‰
        chunk_size: æ¯ä¸ªå—çš„æœ€å¤§è¡Œæ•°
        chunk_overlap: å—ä¹‹é—´çš„é‡å è¡Œæ•°
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è·å–æ‰€æœ‰TXTæ–‡ä»¶
    txt_files = list(input_dir.glob("*.txt"))
    
    if not txt_files:
        print(f"åœ¨ {input_dir} ä¸­æœªæ‰¾åˆ°TXTæ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(txt_files)} ä¸ªTXTæ–‡ä»¶ï¼Œå¼€å§‹è½¬æ¢...")
    
    for txt_file in txt_files:
        print(f"\nå¤„ç†æ–‡ä»¶: {txt_file.name}")
        
        # ç”ŸæˆSHA1
        sha1 = generate_sha1(txt_file)
        
        # æå–å…¬å¸åç§°
        company_name = extract_company_name(txt_file.name)
        
        # åˆ‡åˆ†æ–‡ä»¶
        chunks = split_text_file(txt_file, chunk_size, chunk_overlap)
        
        if not chunks:
            print(f"  è­¦å‘Š: æ–‡ä»¶ {txt_file.name} åˆ‡åˆ†å¤±è´¥")
            continue
        
        # æ„å»ºJSONç»“æ„
        json_data = {
            "metainfo": {
                "sha1": sha1,
                "company_name": company_name,
                "file_name": txt_file.name
            },
            "content": {
                "chunks": chunks
            }
        }
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        output_file = output_dir / f"{txt_file.stem}.json"
        
        # ä¿å­˜JSONæ–‡ä»¶
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            print(f"  æˆåŠŸ: ç”Ÿæˆ {output_file.name} ({len(chunks)} ä¸ªå—)")
            
        except Exception as e:
            print(f"  é”™è¯¯: ä¿å­˜ {output_file.name} å¤±è´¥: {e}")
    
    print(f"\nè½¬æ¢å®Œæˆï¼å…±å¤„ç† {len(txt_files)} ä¸ªæ–‡ä»¶")


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®è·¯å¾„
    current_dir = Path(".")
    input_dir = current_dir / "data" / "extracted_texts"
    output_dir = current_dir / "data" / "json_segments"
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not input_dir.exists():
        print(f"é”™è¯¯: è¾“å…¥ç›®å½• {input_dir} ä¸å­˜åœ¨")
        return
    
    print("=== TXTæ–‡ä»¶è½¬JSONæ ¼å¼åˆ‡åˆ†å·¥å…· ===")
    print(f"è¾“å…¥ç›®å½•: {input_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"åˆ‡åˆ†å‚æ•°: chunk_size=30, chunk_overlap=5")
    print("=" * 50)
    
    # æ‰§è¡Œè½¬æ¢
    convert_txt_to_json(input_dir, output_dir)
    
    print(f"\nâœ… è½¬æ¢å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“Š å¯ä»¥åœ¨è¾“å‡ºç›®å½•ä¸­æŸ¥çœ‹ç”Ÿæˆçš„JSONæ–‡ä»¶")


if __name__ == "__main__":
    main() 